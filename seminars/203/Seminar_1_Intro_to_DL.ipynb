{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Введение. Полносвязные слои. Функции активации (ноутбук)\n",
    "\n",
    "> Начнем осваивать библиотеку `PyTorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## План ноутбука\n",
    "\n",
    "1. Установка `PyTorch`\n",
    "1. Введение в `PyTorch`\n",
    "1. Полносвязные слои и функции активации в `PyTorch`\n",
    "1. Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Установка `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы будем использовать библиотеку для глубинного обучения `PyTorch`, ее можно не устанавливать, можно пользоваться сайтами [Kaggle](kaggle.com) и [Google Colab](colab.research.google.com/) для обучения в облаке (или с учителем?). \n",
    "\n",
    "Чтобы установить `PyTorch` локально себе на компьютер нужно ответить на два вопроса - какая у вас операционная система и есть ли у вас дискретная видеокарта (GPU) и если есть, то какого производителя. В зависимости от ваших ответов мы получаем три варианта по операционной системе - Linux, Mac и Windows; три варианта по дискретной видеокарте - нет видеокарты (доступен только центральный процессор CPU), есть видеокарта от Nvidia или есть видеокарта от AMD (это производитель именно чипа, конечный вендор может быть другой, например, ASUS, MSI, Palit). Работа с PyTorch с видеокартой от AMD это экзотика, которая выходит за рамки нашего курса, поэтому рассмотрим только варианты *нет видеокарты*/*есть видеокарта от Nvidia*.\n",
    "\n",
    "\n",
    "Выберите на [сайте](https://pytorch.org/get-started/locally/) подходящие вам варианты операционной системы/видеокарты и скопируйте команду для установки. Разберем подробно самые популярные варианты установки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Linux ([поддерживаемые дистрибутивы](https://pytorch.org/get-started/locally/#supported-linux-distributions))\n",
    "\n",
    "На линуксе будет работать поддержка `PyTorch` в любой конфигурации, что у вас нет видеокарты, что есть от Nvidia, что от AMD. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Windows\n",
    "\n",
    "На винде будет работать поддержка `PyTorch` только для видеокарт от Nvidia и без видеокарт вообще. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка на Mac\n",
    "\n",
    "На маках есть пока что поддержка `PyTorch` только центрального процессора, чуть позже появится поддержка ускорения на чипах M1, M2, M1 Pro и так далее.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` \n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Введение в `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Тензоры\n",
    "\n",
    "Тензоры — это специализированная структура данных, по сути это массивы и матрицы. Тензоры очень похожи на массивы в numpy, так что, если у вас хорошо с numpy, то разобраться в PyTorch тензорах будет очень просто. В PyTorch мы используем тензоры для кодирования входных и выходных данных модели, а также параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание тензоров\n",
    "\n",
    "Тензор можно создать напрямую из каких-то данных - нам подходят все списки с числами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [1, 2, 3, 4]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[1, 2], [3, 4], [5, 6]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле про \"все\" списки с числами - обман. Если у вашего списка есть какой-то уровень вложенности, то должны совпадать размерности у всех вложенных списков (подробнее про размерности поговорим позже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m some_other_data \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m]]\n\u001b[0;32m----> 2\u001b[0m some_other_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msome_other_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m some_other_tensor\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "some_other_data = [[1, 2], [3, 4], [5, 6, 7]]\n",
    "some_other_tensor = torch.tensor(some_other_data)\n",
    "\n",
    "some_other_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также тензоры можно создавать из numpy массивов и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [2]],\n",
       "\n",
       "       [[3],\n",
       "        [4]],\n",
       "\n",
       "       [[5],\n",
       "        [6]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_numpy_array = np.array(some_data)\n",
    "\n",
    "some_numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor_from_numpy = torch.from_numpy(some_numpy_array)\n",
    "\n",
    "some_tensor_from_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом если мы создаем тензор из numpy массива с помощью `torch.from_numpy`, то они делят между собой память, где лежат их данные и, соответственно, при изменении тензора меняется numpy массив и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10)\n",
    "y = x.numpy()\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем создать тензор со случайными или константными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5285, 0.0950, 0.0681],\n",
       "         [0.1886, 0.5668, 0.0293]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[-1.5984e+24,  4.5567e-41,  1.5823e+24],\n",
       "         [ 3.0744e-41,  4.4842e-44,  0.0000e+00]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "\n",
    "random_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "empty_tensor = torch.empty(shape)\n",
    "\n",
    "random_tensor, ones_tensor, zeros_tensor, empty_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про размерности подробнее.\n",
    "\n",
    "У тензора есть какой-то размер, какая форма. Первое с чем нужно определиться, какой **размерности** тензор - количество осей у него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3225, 0.8161, 0.0535, 0.4004, 0.3338, 0.7905, 0.6220, 0.3950, 0.2429,\n",
       "        0.8542])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (10)  # одна ось (вектор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8274, 0.1927, 0.1838],\n",
       "        [0.9486, 0.1779, 0.0403]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)  # две оси (матрица)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9068, 0.0370, 0.7538],\n",
       "         [0.8729, 0.3910, 0.9743]],\n",
       "\n",
       "        [[0.7231, 0.4850, 0.9361],\n",
       "         [0.3390, 0.6760, 0.4937]],\n",
       "\n",
       "        [[0.2165, 0.1672, 0.0126],\n",
       "         [0.0113, 0.4041, 0.8066]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (3, 2, 3)  # три оси (и больше - тензор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензор с размерностью 1 - это просто вектор, список чисел.\n",
    "\n",
    "Тензор с размерностью 2 - это просто матрица, то есть список списков чисел.\n",
    "\n",
    "Тензор с размерностью 3 и больше - это тензор, то есть список списков списков ... чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить доступ к размеру уже созданного тензора - метод `.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]],\n",
      "\n",
      "        [[5],\n",
      "         [6]]])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "print(some_tensor)\n",
    "print(some_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первом семинаре по мо мы говорили про изображения, давайте сделаем тензор, который будет нам имитировать изображение - сделаем его размер `(c, h, w)`, где `h` и `w` это его высота и ширина, а `c` - число каналов в цветовом пространстве (в черно-белом 1, в RGB 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9775, 0.2982, 0.6353, 0.3182, 0.9650, 0.6874, 0.4357, 0.3855,\n",
       "          0.2320, 0.9186, 0.0501, 0.8103, 0.8994, 0.2158, 0.6888, 0.7848],\n",
       "         [0.4642, 0.9638, 0.8255, 0.8171, 0.7430, 0.5249, 0.9075, 0.9157,\n",
       "          0.6988, 0.1453, 0.6026, 0.5855, 0.8902, 0.0364, 0.3849, 0.6432],\n",
       "         [0.1201, 0.3256, 0.9966, 0.9750, 0.5007, 0.5886, 0.9798, 0.7002,\n",
       "          0.3306, 0.4589, 0.9681, 0.0303, 0.5265, 0.9350, 0.0633, 0.4344],\n",
       "         [0.0536, 0.1132, 0.4557, 0.7181, 0.6780, 0.4703, 0.9379, 0.4447,\n",
       "          0.7742, 0.5046, 0.7178, 0.1696, 0.6314, 0.6709, 0.1635, 0.8478],\n",
       "         [0.2710, 0.0826, 0.1818, 0.9011, 0.3186, 0.4310, 0.6464, 0.2153,\n",
       "          0.3934, 0.6579, 0.5010, 0.0092, 0.5486, 0.6124, 0.4359, 0.5813],\n",
       "         [0.8364, 0.4378, 0.9768, 0.6568, 0.9091, 0.9551, 0.7631, 0.6603,\n",
       "          0.2461, 0.2755, 0.3421, 0.3058, 0.2876, 0.5780, 0.1062, 0.7334],\n",
       "         [0.9061, 0.0728, 0.6684, 0.3918, 0.1079, 0.6306, 0.4478, 0.6679,\n",
       "          0.5032, 0.6860, 0.7216, 0.7122, 0.6370, 0.8657, 0.7830, 0.6122],\n",
       "         [0.3057, 0.9417, 0.1864, 0.3049, 0.5392, 0.4781, 0.5376, 0.7038,\n",
       "          0.9354, 0.5136, 0.9475, 0.0622, 0.5721, 0.1706, 0.0551, 0.6496],\n",
       "         [0.8474, 0.2614, 0.5602, 0.6474, 0.3625, 0.4565, 0.9942, 0.8083,\n",
       "          0.6352, 0.8240, 0.4011, 0.9550, 0.7343, 0.0541, 0.2522, 0.9094]],\n",
       "\n",
       "        [[0.3925, 0.1882, 0.7496, 0.3814, 0.3472, 0.5002, 0.8938, 0.8759,\n",
       "          0.2210, 0.1095, 0.7503, 0.8389, 0.5584, 0.8224, 0.3736, 0.2216],\n",
       "         [0.1921, 0.2562, 0.5169, 0.5303, 0.2085, 0.6784, 0.9823, 0.4100,\n",
       "          0.9329, 0.5424, 0.2917, 0.8005, 0.7115, 0.6089, 0.7245, 0.1239],\n",
       "         [0.0529, 0.5003, 0.3232, 0.3429, 0.1243, 0.4324, 0.5462, 0.2154,\n",
       "          0.8968, 0.6407, 0.3528, 0.5638, 0.8849, 0.7607, 0.7689, 0.5715],\n",
       "         [0.4333, 0.4159, 0.5505, 0.1750, 0.1556, 0.8159, 0.8702, 0.0758,\n",
       "          0.7108, 0.8237, 0.8938, 0.0610, 0.5502, 0.9200, 0.7361, 0.6637],\n",
       "         [0.2515, 0.8768, 0.3475, 0.8666, 0.9651, 0.0841, 0.3307, 0.4528,\n",
       "          0.8748, 0.2424, 0.9168, 0.4830, 0.6686, 0.5376, 0.3726, 0.9477],\n",
       "         [0.5003, 0.4306, 0.5180, 0.3192, 0.5477, 0.9518, 0.8254, 0.6865,\n",
       "          0.9612, 0.9703, 0.5021, 0.7801, 0.7338, 0.1218, 0.7767, 0.8723],\n",
       "         [0.2581, 0.9353, 0.2863, 0.4518, 0.8973, 0.8795, 0.3254, 0.8512,\n",
       "          0.0479, 0.1487, 0.5186, 0.8087, 0.2621, 0.9327, 0.8455, 0.2348],\n",
       "         [0.9443, 0.9300, 0.9501, 0.6541, 0.7211, 0.9584, 0.6674, 0.7161,\n",
       "          0.7434, 0.4748, 0.7699, 0.9569, 0.0600, 0.9372, 0.9299, 0.4137],\n",
       "         [0.4388, 0.9522, 0.9600, 0.3329, 0.3210, 0.0522, 0.9013, 0.5496,\n",
       "          0.3224, 0.7553, 0.3364, 0.4721, 0.5902, 0.8506, 0.8019, 0.7434]],\n",
       "\n",
       "        [[0.4258, 0.7125, 0.2274, 0.3973, 0.7817, 0.2999, 0.1742, 0.2642,\n",
       "          0.7853, 0.6137, 0.9008, 0.4814, 0.2827, 0.4540, 0.0580, 0.7212],\n",
       "         [0.4462, 0.8208, 0.9613, 0.5641, 0.7291, 0.2954, 0.3707, 0.5401,\n",
       "          0.2302, 0.5194, 0.1200, 0.8744, 0.0386, 0.4592, 0.4798, 0.4317],\n",
       "         [0.0101, 0.5234, 0.0215, 0.3943, 0.7812, 0.5038, 0.0204, 0.1198,\n",
       "          0.1225, 0.4308, 0.8900, 0.0152, 0.4555, 0.2082, 0.3759, 0.9852],\n",
       "         [0.8497, 0.7731, 0.4931, 0.9246, 0.1300, 0.4181, 0.2902, 0.9975,\n",
       "          0.0480, 0.6976, 0.7330, 0.1745, 0.3693, 0.4605, 0.6504, 0.5115],\n",
       "         [0.3773, 0.1451, 0.2474, 0.0038, 0.9771, 0.1710, 0.0546, 0.8336,\n",
       "          0.6072, 0.7608, 0.4589, 0.9116, 0.3840, 0.1267, 0.7785, 0.4884],\n",
       "         [0.9098, 0.8704, 0.0794, 0.4136, 0.7548, 0.5899, 0.0590, 0.5794,\n",
       "          0.8823, 0.2317, 0.8598, 0.5015, 0.3090, 0.3907, 0.5748, 0.9424],\n",
       "         [0.9353, 0.0947, 0.7538, 0.8242, 0.4496, 0.1007, 0.7796, 0.1501,\n",
       "          0.1892, 0.4765, 0.3393, 0.9327, 0.0313, 0.6508, 0.4608, 0.7992],\n",
       "         [0.2431, 0.9116, 0.8328, 0.3738, 0.0745, 0.2764, 0.9508, 0.7395,\n",
       "          0.8753, 0.0855, 0.8216, 0.5617, 0.7598, 0.5998, 0.0806, 0.5730],\n",
       "         [0.4534, 0.4837, 0.7669, 0.4347, 0.7881, 0.8379, 0.7061, 0.1766,\n",
       "          0.7569, 0.5044, 0.8774, 0.4793, 0.9651, 0.8613, 0.7067, 0.5286]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 9\n",
    "w = 16\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем попробовать поменять размер тензора, например, [вытянуть его в вектор](https://pytorch.org/docs/stable/generated/torch.ravel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9775, 0.2982, 0.6353, 0.3182, 0.9650, 0.6874, 0.4357, 0.3855, 0.2320,\n",
       "        0.9186, 0.0501, 0.8103, 0.8994, 0.2158, 0.6888, 0.7848, 0.4642, 0.9638,\n",
       "        0.8255, 0.8171, 0.7430, 0.5249, 0.9075, 0.9157, 0.6988, 0.1453, 0.6026,\n",
       "        0.5855, 0.8902, 0.0364, 0.3849, 0.6432, 0.1201, 0.3256, 0.9966, 0.9750,\n",
       "        0.5007, 0.5886, 0.9798, 0.7002, 0.3306, 0.4589, 0.9681, 0.0303, 0.5265,\n",
       "        0.9350, 0.0633, 0.4344, 0.0536, 0.1132, 0.4557, 0.7181, 0.6780, 0.4703,\n",
       "        0.9379, 0.4447, 0.7742, 0.5046, 0.7178, 0.1696, 0.6314, 0.6709, 0.1635,\n",
       "        0.8478, 0.2710, 0.0826, 0.1818, 0.9011, 0.3186, 0.4310, 0.6464, 0.2153,\n",
       "        0.3934, 0.6579, 0.5010, 0.0092, 0.5486, 0.6124, 0.4359, 0.5813, 0.8364,\n",
       "        0.4378, 0.9768, 0.6568, 0.9091, 0.9551, 0.7631, 0.6603, 0.2461, 0.2755,\n",
       "        0.3421, 0.3058, 0.2876, 0.5780, 0.1062, 0.7334, 0.9061, 0.0728, 0.6684,\n",
       "        0.3918, 0.1079, 0.6306, 0.4478, 0.6679, 0.5032, 0.6860, 0.7216, 0.7122,\n",
       "        0.6370, 0.8657, 0.7830, 0.6122, 0.3057, 0.9417, 0.1864, 0.3049, 0.5392,\n",
       "        0.4781, 0.5376, 0.7038, 0.9354, 0.5136, 0.9475, 0.0622, 0.5721, 0.1706,\n",
       "        0.0551, 0.6496, 0.8474, 0.2614, 0.5602, 0.6474, 0.3625, 0.4565, 0.9942,\n",
       "        0.8083, 0.6352, 0.8240, 0.4011, 0.9550, 0.7343, 0.0541, 0.2522, 0.9094,\n",
       "        0.3925, 0.1882, 0.7496, 0.3814, 0.3472, 0.5002, 0.8938, 0.8759, 0.2210,\n",
       "        0.1095, 0.7503, 0.8389, 0.5584, 0.8224, 0.3736, 0.2216, 0.1921, 0.2562,\n",
       "        0.5169, 0.5303, 0.2085, 0.6784, 0.9823, 0.4100, 0.9329, 0.5424, 0.2917,\n",
       "        0.8005, 0.7115, 0.6089, 0.7245, 0.1239, 0.0529, 0.5003, 0.3232, 0.3429,\n",
       "        0.1243, 0.4324, 0.5462, 0.2154, 0.8968, 0.6407, 0.3528, 0.5638, 0.8849,\n",
       "        0.7607, 0.7689, 0.5715, 0.4333, 0.4159, 0.5505, 0.1750, 0.1556, 0.8159,\n",
       "        0.8702, 0.0758, 0.7108, 0.8237, 0.8938, 0.0610, 0.5502, 0.9200, 0.7361,\n",
       "        0.6637, 0.2515, 0.8768, 0.3475, 0.8666, 0.9651, 0.0841, 0.3307, 0.4528,\n",
       "        0.8748, 0.2424, 0.9168, 0.4830, 0.6686, 0.5376, 0.3726, 0.9477, 0.5003,\n",
       "        0.4306, 0.5180, 0.3192, 0.5477, 0.9518, 0.8254, 0.6865, 0.9612, 0.9703,\n",
       "        0.5021, 0.7801, 0.7338, 0.1218, 0.7767, 0.8723, 0.2581, 0.9353, 0.2863,\n",
       "        0.4518, 0.8973, 0.8795, 0.3254, 0.8512, 0.0479, 0.1487, 0.5186, 0.8087,\n",
       "        0.2621, 0.9327, 0.8455, 0.2348, 0.9443, 0.9300, 0.9501, 0.6541, 0.7211,\n",
       "        0.9584, 0.6674, 0.7161, 0.7434, 0.4748, 0.7699, 0.9569, 0.0600, 0.9372,\n",
       "        0.9299, 0.4137, 0.4388, 0.9522, 0.9600, 0.3329, 0.3210, 0.0522, 0.9013,\n",
       "        0.5496, 0.3224, 0.7553, 0.3364, 0.4721, 0.5902, 0.8506, 0.8019, 0.7434,\n",
       "        0.4258, 0.7125, 0.2274, 0.3973, 0.7817, 0.2999, 0.1742, 0.2642, 0.7853,\n",
       "        0.6137, 0.9008, 0.4814, 0.2827, 0.4540, 0.0580, 0.7212, 0.4462, 0.8208,\n",
       "        0.9613, 0.5641, 0.7291, 0.2954, 0.3707, 0.5401, 0.2302, 0.5194, 0.1200,\n",
       "        0.8744, 0.0386, 0.4592, 0.4798, 0.4317, 0.0101, 0.5234, 0.0215, 0.3943,\n",
       "        0.7812, 0.5038, 0.0204, 0.1198, 0.1225, 0.4308, 0.8900, 0.0152, 0.4555,\n",
       "        0.2082, 0.3759, 0.9852, 0.8497, 0.7731, 0.4931, 0.9246, 0.1300, 0.4181,\n",
       "        0.2902, 0.9975, 0.0480, 0.6976, 0.7330, 0.1745, 0.3693, 0.4605, 0.6504,\n",
       "        0.5115, 0.3773, 0.1451, 0.2474, 0.0038, 0.9771, 0.1710, 0.0546, 0.8336,\n",
       "        0.6072, 0.7608, 0.4589, 0.9116, 0.3840, 0.1267, 0.7785, 0.4884, 0.9098,\n",
       "        0.8704, 0.0794, 0.4136, 0.7548, 0.5899, 0.0590, 0.5794, 0.8823, 0.2317,\n",
       "        0.8598, 0.5015, 0.3090, 0.3907, 0.5748, 0.9424, 0.9353, 0.0947, 0.7538,\n",
       "        0.8242, 0.4496, 0.1007, 0.7796, 0.1501, 0.1892, 0.4765, 0.3393, 0.9327,\n",
       "        0.0313, 0.6508, 0.4608, 0.7992, 0.2431, 0.9116, 0.8328, 0.3738, 0.0745,\n",
       "        0.2764, 0.9508, 0.7395, 0.8753, 0.0855, 0.8216, 0.5617, 0.7598, 0.5998,\n",
       "        0.0806, 0.5730, 0.4534, 0.4837, 0.7669, 0.4347, 0.7881, 0.8379, 0.7061,\n",
       "        0.1766, 0.7569, 0.5044, 0.8774, 0.4793, 0.9651, 0.8613, 0.7067, 0.5286])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([432])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h * w * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество элементов в тензоре с помощью [специальной функции](https://pytorch.org/docs/stable/generated/torch.numel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3933, 0.9872, 0.5547],\n",
       "         [0.1623, 0.0614, 0.4035]],\n",
       "\n",
       "        [[0.1474, 0.3112, 0.6072],\n",
       "         [0.1661, 0.8004, 0.8886]],\n",
       "\n",
       "        [[0.4359, 0.2741, 0.9206],\n",
       "         [0.6655, 0.5385, 0.1498]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 2\n",
    "w = 3\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поменять размер с помощью функции [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3933, 0.9872, 0.5547, 0.1623, 0.0614, 0.4035],\n",
       "        [0.1474, 0.3112, 0.6072, 0.1661, 0.8004, 0.8886],\n",
       "        [0.4359, 0.2741, 0.9206, 0.6655, 0.5385, 0.1498]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.reshape(c, h * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем собрать из нескольких тензоров один большой:\n",
    "\n",
    "[torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5783, -0.7873, -0.3202],\n",
       "        [-0.3062, -0.6686, -2.3031]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5783, -0.7873, -0.3202],\n",
       "        [-0.3062, -0.6686, -2.3031],\n",
       "        [ 1.5783, -0.7873, -0.3202],\n",
       "        [-0.3062, -0.6686, -2.3031],\n",
       "        [ 1.5783, -0.7873, -0.3202],\n",
       "        [-0.3062, -0.6686, -2.3031]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5783, -0.7873, -0.3202,  1.5783, -0.7873, -0.3202,  1.5783, -0.7873,\n",
       "         -0.3202],\n",
       "        [-0.3062, -0.6686, -2.3031, -0.3062, -0.6686, -2.3031, -0.3062, -0.6686,\n",
       "         -2.3031]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1104, -0.7963, -0.1114],\n",
      "        [ 0.3345, -0.6622,  0.0195],\n",
      "        [ 0.6688,  1.2556, -0.5008]])\n",
      "tensor([[ 0.5901,  2.1711,  0.9652],\n",
      "        [ 0.5086,  1.3394, -2.4193],\n",
      "        [-1.6633, -0.8344, -0.6417],\n",
      "        [ 0.6202, -0.6315, -0.8484],\n",
      "        [-0.2572, -0.3911,  2.0413]])\n",
      "tensor([[ 0.1756,  1.6419, -0.0950]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1104, -0.7963, -0.1114],\n",
       "        [ 0.3345, -0.6622,  0.0195],\n",
       "        [ 0.6688,  1.2556, -0.5008],\n",
       "        [ 0.5901,  2.1711,  0.9652],\n",
       "        [ 0.5086,  1.3394, -2.4193],\n",
       "        [-1.6633, -0.8344, -0.6417],\n",
       "        [ 0.6202, -0.6315, -0.8484],\n",
       "        [-0.2572, -0.3911,  2.0413],\n",
       "        [ 0.1756,  1.6419, -0.0950]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(5, 3)\n",
    "z = torch.randn(1, 3)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0439,  0.3354, -0.9225],\n",
      "        [ 0.8540,  0.7514,  1.6939]])\n",
      "tensor([[-1.1206, -0.1131,  0.3248, -1.2019, -0.6680],\n",
      "        [ 1.9606, -0.7643,  1.1579,  1.8466,  1.4457]])\n",
      "tensor([[ 0.8653],\n",
      "        [-0.9537]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0439,  0.3354, -0.9225, -1.1206, -0.1131,  0.3248, -1.2019, -0.6680,\n",
       "          0.8653],\n",
       "        [ 0.8540,  0.7514,  1.6939,  1.9606, -0.7643,  1.1579,  1.8466,  1.4457,\n",
       "         -0.9537]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 5)\n",
    "z = torch.randn(2, 1)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим дополнительную ось:\n",
    "\n",
    "[torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5329, 0.0917, 0.6932],\n",
      "        [0.9196, 0.5841, 0.9417]])\n",
      "\n",
      "tensor([[[0.5329, 0.0917, 0.6932],\n",
      "         [0.9196, 0.5841, 0.9417]]]) torch.Size([1, 2, 3])\n",
      "\n",
      "tensor([[[0.5329, 0.0917, 0.6932]],\n",
      "\n",
      "        [[0.9196, 0.5841, 0.9417]]]) torch.Size([2, 1, 3])\n",
      "\n",
      "tensor([[[0.5329],\n",
      "         [0.0917],\n",
      "         [0.6932]],\n",
      "\n",
      "        [[0.9196],\n",
      "         [0.5841],\n",
      "         [0.9417]]]) torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.unsqueeze(0), x.unsqueeze(0).shape)\n",
    "print()\n",
    "print(x.unsqueeze(1), x.unsqueeze(1).shape)\n",
    "print()\n",
    "print(x.unsqueeze(2), x.unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем лишние оси (где размер единичка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6917, 0.4355, 0.0841]],\n",
      "\n",
      "         [[0.7397, 0.2567, 0.1006]]]])\n",
      "\n",
      "tensor([[0.6917, 0.4355, 0.0841],\n",
      "        [0.7397, 0.2567, 0.1006]]) torch.Size([2, 3])\n",
      "\n",
      "tensor([[[0.6917, 0.4355, 0.0841]],\n",
      "\n",
      "        [[0.7397, 0.2567, 0.1006]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2, 1, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.squeeze(), x.squeeze().shape)\n",
    "print()\n",
    "print(x.squeeze(0), x.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про типы данных в тензорах. По умолчанию в тензорах лежат числа в torch.float32 для вещественных и torch.int64 для целочисленных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.1992, 3.6992, 4.8984], dtype=torch.float16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float64)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int32)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размещение тензора на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 27 13:05:56 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:09:00.0  On |                    0 |\r\n",
      "| 30%   32C    P8    30W / 450W |    906MiB / 23028MiB |     41%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2254      G   /usr/lib/xorg/Xorg                687MiB |\r\n",
      "|    0   N/A  N/A      2692      G   /usr/bin/gnome-shell              102MiB |\r\n",
      "|    0   N/A  N/A      7560      G   ...RendererForSitePerProcess       66MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], device=device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 22, 37, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor.cpu()\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1576, 0.6617, 1.5070],\n",
       "        [1.5950, 1.3520, 0.6221]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "a = a.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1576, 0.6617, 1.5070],\n",
       "        [1.5950, 1.3520, 0.6221]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Операции с тензорами\n",
    "\n",
    "Большая часть операций с тензорами хорошо описана в их [документации](https://pytorch.org/docs/stable/torch.html), разберем основные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7609, 0.5646, 0.7532],\n",
       "         [0.7134, 0.2503, 0.9244]]),\n",
       " tensor([[0.7039, 0.9685, 0.5335],\n",
       "         [0.4674, 0.4437, 0.2221]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4647, 1.5332, 1.2867],\n",
      "        [1.1808, 0.6940, 1.1465]])\n",
      "\n",
      "tensor([[1.4647, 1.5332, 1.2867],\n",
      "        [1.1808, 0.6940, 1.1465]])\n",
      "\n",
      "tensor([[1.4647, 1.5332, 1.2867],\n",
      "        [1.1808, 0.6940, 1.1465]])\n"
     ]
    }
   ],
   "source": [
    "# поэлементные\n",
    "\n",
    "print(a + b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.add(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.add(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0570, -0.4039,  0.2198],\n",
      "        [ 0.2461, -0.1933,  0.7023]])\n",
      "\n",
      "tensor([[ 0.0570, -0.4039,  0.2198],\n",
      "        [ 0.2461, -0.1933,  0.7023]])\n",
      "\n",
      "tensor([[ 0.0570, -0.4039,  0.2198],\n",
      "        [ 0.2461, -0.1933,  0.7023]])\n"
     ]
    }
   ],
   "source": [
    "print(a - b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.sub(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.sub(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5356, 0.5469, 0.4018],\n",
      "        [0.3334, 0.1111, 0.2053]])\n",
      "\n",
      "tensor([[0.5356, 0.5469, 0.4018],\n",
      "        [0.3334, 0.1111, 0.2053]])\n",
      "\n",
      "tensor([[0.5356, 0.5469, 0.4018],\n",
      "        [0.3334, 0.1111, 0.2053]])\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.mul(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.mul(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0810, 0.5830, 1.4120],\n",
      "        [1.5265, 0.5643, 4.1628]])\n",
      "\n",
      "tensor([[1.0810, 0.5830, 1.4120],\n",
      "        [1.5265, 0.5643, 4.1628]])\n",
      "\n",
      "tensor([[1.0810, 0.5830, 1.4120],\n",
      "        [1.5265, 0.5643, 4.1628]])\n"
     ]
    }
   ],
   "source": [
    "print(a / b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.div(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.div(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0460, 0.1144, 0.0167],\n",
       "         [0.9431, 0.6636, 0.9862]]),\n",
       " tensor([[0.8875, 0.6632, 0.0202, 0.3087],\n",
       "         [0.5831, 0.9169, 0.0202, 0.8303],\n",
       "         [0.8097, 0.6054, 0.7213, 0.7455]]),\n",
       " tensor([[0.3923, 0.7233, 0.2330, 0.1275, 0.2274],\n",
       "         [0.0384, 0.0557, 0.9578, 0.1508, 0.4922],\n",
       "         [0.2012, 0.1229, 0.5835, 0.5487, 0.2447],\n",
       "         [0.2705, 0.4000, 0.0566, 0.4120, 0.6579],\n",
       "         [0.2921, 0.5955, 0.3066, 0.8832, 0.8277]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 4)\n",
    "c = torch.rand(5, 5)\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1210, 0.1455, 0.0153, 0.1216],\n",
      "        [2.0225, 1.8310, 0.7437, 1.5773]]) torch.Size([2, 4])\n",
      "\n",
      "tensor([[0.1210, 0.1455, 0.0153, 0.1216],\n",
      "        [2.0225, 1.8310, 0.7437, 1.5773]]) torch.Size([2, 4])\n",
      "\n",
      "tensor(2.2712)\n",
      "\n",
      "tensor([[1.4804, 2.0613, 1.2624, 1.1360, 1.2553],\n",
      "        [1.0391, 1.0573, 2.6060, 1.1627, 1.6359],\n",
      "        [1.2228, 1.1308, 1.7923, 1.7310, 1.2772],\n",
      "        [1.3106, 1.4919, 1.0583, 1.5098, 1.9307],\n",
      "        [1.3393, 1.8140, 1.3588, 2.4186, 2.2880]])\n"
     ]
    }
   ],
   "source": [
    "# матричные операции\n",
    "\n",
    "print(a @ b, (a @ b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.matmul(a, b), torch.matmul(a, b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.trace())\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### [Автоматическое дифференцирование](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3343, 0.0496, 0.7883, 0.4295, 0.2165])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4772, 0.4433, 0.0935, 0.0830, 0.6375],\n",
       "        [0.8642, 0.6238, 0.1696, 0.7868, 0.9153],\n",
       "        [0.4401, 0.9950, 0.0673, 0.0670, 0.8410]], requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_z = torch.empty(3)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4289, 0.9896, 0.4603], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    first_z[i] = torch.sum(w[i] * x)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4289, 0.9896, 0.4603], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w.t())\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1376, 0.2297, 0.3049], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand(3, requires_grad=True)\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4267, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sum(z * v)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4267200231552124"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((y - 2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4752, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=None\n",
      "\n",
      "z=tensor([0.4289, 0.9896, 0.4603], grad_fn=<SqueezeBackward3>)\n",
      "\n",
      "v.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=tensor([[-0.1448, -0.0215, -0.3413, -0.1860, -0.0937],\n",
      "        [-0.2417, -0.0358, -0.5698, -0.3105, -0.1565],\n",
      "        [-0.3208, -0.0475, -0.7562, -0.4120, -0.2077]])\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=tensor([-1.3494, -3.1138, -1.4485])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuke/.local/lib/python3.10/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5212], requires_grad=True), tensor([0.9553], requires_grad=True))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4341], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([1.])\n",
      "\n",
      "b.grad=tensor([-1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 1\n",
    "print(f'{b.grad=}\\n')  # -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1884], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b) ** 2\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.])\n",
      "\n",
      "b.grad=tensor([0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([-0.8681])\n",
      "\n",
      "b.grad=tensor([0.8681])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 2 * (a - b)\n",
    "print(f'{b.grad=}\\n')  # -2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8681], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1162, 0.8336, 0.3941, 0.3027, 0.3148],\n",
       "         [0.8559, 0.2273, 0.5742, 0.4006, 0.4817],\n",
       "         [0.8545, 0.5922, 0.5382, 0.4817, 0.1110]], requires_grad=True),\n",
       " tensor([[0.8097, 0.7841, 0.0872, 0.5702, 0.1409],\n",
       "         [0.3843, 0.8032, 0.7116, 0.0247, 0.4326],\n",
       "         [0.9113, 0.9081, 0.8865, 0.2243, 0.7870]], requires_grad=True))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2751, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean(a * b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[0.0540, 0.0523, 0.0058, 0.0380, 0.0094],\n",
      "        [0.0256, 0.0535, 0.0474, 0.0016, 0.0288],\n",
      "        [0.0608, 0.0605, 0.0591, 0.0150, 0.0525]])\n",
      "\n",
      "b.grad=tensor([[0.0077, 0.0556, 0.0263, 0.0202, 0.0210],\n",
      "        [0.0571, 0.0152, 0.0383, 0.0267, 0.0321],\n",
      "        [0.0570, 0.0395, 0.0359, 0.0321, 0.0074]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # b / (3 * 5)\n",
    "print(f'{b.grad=}\\n')  # a / (3 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0077, 0.0556, 0.0263, 0.0202, 0.0210],\n",
       "        [0.0571, 0.0152, 0.0383, 0.0267, 0.0321],\n",
       "        [0.0570, 0.0395, 0.0359, 0.0321, 0.0074]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0540, 0.0523, 0.0058, 0.0380, 0.0094],\n",
       "        [0.0256, 0.0535, 0.0474, 0.0016, 0.0288],\n",
       "        [0.0608, 0.0605, 0.0591, 0.0150, 0.0525]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[0.1367, 0.2009, 0.9751, 0.0827, 0.4974],\n",
      "        [0.9151, 0.3569, 0.2004, 0.7528, 0.0558],\n",
      "        [0.6241, 0.2074, 0.9411, 0.3785, 0.4339]], requires_grad=True)\n",
      "\n",
      "a.grad=None\n",
      "\n",
      "a.grad=tensor([[0.2733, 0.4017, 1.9501, 0.1654, 0.9948],\n",
      "        [1.8302, 0.7139, 0.4007, 1.5056, 0.1116],\n",
      "        [1.2483, 0.4148, 1.8823, 0.7571, 0.8679]])\n",
      "\n",
      "a.grad=tensor([[1.2733, 1.4017, 2.9501, 1.1654, 1.9948],\n",
      "        [2.8302, 1.7139, 1.4007, 2.5056, 1.1116],\n",
      "        [2.2483, 1.4148, 2.8823, 1.7571, 1.8679]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "print(f'{a=}\\n')\n",
    "\n",
    "loss1 = torch.sum(a ** 2) # 2a\n",
    "loss2 = torch.sum(a) # 1\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss1.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss2.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*a=tensor([[0.2733, 0.4017, 1.9501, 0.1654, 0.9948],\n",
      "        [1.8302, 0.7139, 0.4007, 1.5056, 0.1116],\n",
      "        [1.2483, 0.4148, 1.8823, 0.7571, 0.8679]], grad_fn=<MulBackward0>)\n",
      "\n",
      "2*a+1=tensor([[1.2733, 1.4017, 2.9501, 1.1654, 1.9948],\n",
      "        [2.8302, 1.7139, 1.4007, 2.5056, 1.1116],\n",
      "        [2.2483, 1.4148, 2.8823, 1.7571, 1.8679]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'{2*a=}\\n')\n",
    "print(f'{2*a+1=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9742, 0.8412, 0.1956, 0.8664, 0.7409],\n",
       "         [0.7795, 0.6947, 0.1423, 0.1107, 0.7531],\n",
       "         [0.7261, 0.0423, 0.6392, 0.8209, 0.1107]], requires_grad=True),\n",
       " tensor([[0.6061, 0.7807, 0.9728, 0.8722, 0.2315],\n",
       "         [0.4724, 0.8559, 0.1686, 0.8314, 0.3153],\n",
       "         [0.9036, 0.6909, 0.9585, 0.8875, 0.3550]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=False)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.4645, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # all ones\n",
    "print(f'{b.grad=}\\n')  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6591, 0.3559, 0.4382, 0.7858, 0.5650],\n",
       "         [0.1240, 0.3820, 0.9481, 0.5570, 0.0233],\n",
       "         [0.5378, 0.7493, 0.7096, 0.6130, 0.8606]], requires_grad=True),\n",
       " tensor([[0.2429, 0.8651, 0.3318, 0.3435, 0.5525],\n",
       "         [0.6306, 0.4984, 0.5421, 0.5289, 0.4043],\n",
       "         [0.6634, 0.4914, 0.9217, 0.4506, 0.4609]], requires_grad=True))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3808)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [101]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6759, 0.8305, 0.0023, 0.0310, 0.3375],\n",
       "         [0.4710, 0.1703, 0.5113, 0.7140, 0.9330],\n",
       "         [0.3122, 0.3962, 0.1183, 0.7977, 0.1707]], requires_grad=True),\n",
       " tensor([[0.7970, 0.5161, 0.1340, 0.9999, 0.0532],\n",
       "         [0.9287, 0.4391, 0.5212, 0.8420, 0.2240],\n",
       "         [0.4305, 0.9469, 0.9879, 0.8382, 0.7450]], requires_grad=True))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.9318)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(16.0522)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.0522, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(16.4196)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4196)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(0.8285)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1213, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.0243)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0896)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Полносвязные слои и функции активации в `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Полносвязный слой\n",
    "\n",
    ">$y_j = \\sum\\limits_{i=1}^{n}x_iw_{ji} + b_j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0804,  0.4169, -0.1886,  0.2941,  0.0236],\n",
       "        [-0.0150, -0.0498, -0.0415,  0.0346, -0.3180],\n",
       "        [ 0.3701,  0.1745,  0.3964,  0.1394, -0.2358]], requires_grad=True)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3460, -0.2148, -0.0218], requires_grad=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module._call_impl of Linear(in_features=5, out_features=3, bias=False)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0219,  0.3310, -0.3307], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2783], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=5, out_features=3)\n",
    "layer2 = nn.Linear(in_features=3, out_features=1)\n",
    "\n",
    "layer2(layer1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Сигмоида $f(x) = \\dfrac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5426, -0.2482, -0.1976, -0.7524, -0.4710])\n",
      "tensor([0.3676, 0.4383, 0.4508, 0.3203, 0.3844])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ReLU $f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2238,  1.8375, -0.7093, -1.5693,  1.5010])\n",
      "tensor([0.0000, 1.8375, 0.0000, 0.0000, 1.5010])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Leaky ReLU $f(x) = \\max(0, x) + \\alpha \\min(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.LeakyReLU(negative_slope=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1451, 0.0643, 1.1392, 0.8170, 1.0964])\n",
      "tensor([1.1451, 0.0643, 1.1392, 0.8170, 1.0964])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5511], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=5, out_features=3)\n",
    "activation = nn.LeakyReLU(negative_slope=0.001)\n",
    "layer2 = nn.Linear(in_features=3, out_features=1)\n",
    "\n",
    "layer2(activation(layer1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "b_true = torch.randn(1)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects) + b_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 2])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "step_size = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 132.90622\n",
      "MSE на шаге 2 49.35909\n",
      "MSE на шаге 3 22.88507\n",
      "MSE на шаге 4 14.07332\n",
      "MSE на шаге 5 10.82310\n",
      "MSE на шаге 6 9.39019\n",
      "MSE на шаге 7 8.59645\n",
      "MSE на шаге 8 8.05784\n",
      "MSE на шаге 9 7.64028\n",
      "MSE на шаге 10 7.29122\n",
      "MSE на шаге 11 6.98656\n",
      "MSE на шаге 12 6.71334\n",
      "MSE на шаге 13 6.46372\n",
      "MSE на шаге 14 6.23256\n",
      "MSE на шаге 15 6.01635\n",
      "MSE на шаге 16 5.81262\n",
      "MSE на шаге 17 5.61962\n",
      "MSE на шаге 18 5.43605\n",
      "MSE на шаге 19 5.26093\n",
      "MSE на шаге 20 5.09355\n",
      "MSE на шаге 21 4.93330\n",
      "MSE на шаге 31 3.64659\n",
      "MSE на шаге 41 2.78460\n",
      "MSE на шаге 51 2.20579\n",
      "MSE на шаге 61 1.81710\n",
      "MSE на шаге 71 1.55609\n",
      "MSE на шаге 81 1.38080\n",
      "MSE на шаге 91 1.26310\n",
      "MSE на шаге 101 1.18405\n",
      "MSE на шаге 111 1.13097\n",
      "MSE на шаге 121 1.09532\n",
      "MSE на шаге 131 1.07139\n",
      "MSE на шаге 141 1.05531\n",
      "MSE на шаге 151 1.04452\n",
      "MSE на шаге 161 1.03727\n",
      "MSE на шаге 171 1.03240\n",
      "MSE на шаге 181 1.02913\n",
      "MSE на шаге 191 1.02694\n"
     ]
    }
   ],
   "source": [
    "w = torch.rand(n_features, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = torch.matmul(x, w) + b\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "    \n",
    "#     print(f'{w.grad=}\\n')\n",
    "#     print(f'{b.grad=}\\n')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * step_size\n",
    "        b -= b.grad * step_size\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 61.95393\n",
      "MSE на шаге 2 44.36054\n",
      "MSE на шаге 3 38.58072\n",
      "MSE на шаге 4 36.45937\n",
      "MSE на шаге 5 35.49417\n",
      "MSE на шаге 6 34.91190\n",
      "MSE на шаге 7 34.46986\n",
      "MSE на шаге 8 34.08925\n",
      "MSE на шаге 9 33.74268\n",
      "MSE на шаге 10 33.41941\n",
      "MSE на шаге 11 33.11440\n",
      "MSE на шаге 12 32.82486\n",
      "MSE на шаге 13 32.54897\n",
      "MSE на шаге 14 32.28542\n",
      "MSE на шаге 15 32.03323\n",
      "MSE на шаге 16 31.79161\n",
      "MSE на шаге 17 31.55992\n",
      "MSE на шаге 18 31.33762\n",
      "MSE на шаге 19 31.12423\n",
      "MSE на шаге 20 30.91933\n",
      "MSE на шаге 21 30.72254\n",
      "MSE на шаге 31 29.13294\n",
      "MSE на шаге 41 28.06584\n",
      "MSE на шаге 51 27.34926\n",
      "MSE на шаге 61 26.86805\n",
      "MSE на шаге 71 26.54490\n",
      "MSE на шаге 81 26.32790\n",
      "MSE на шаге 91 26.18217\n",
      "MSE на шаге 101 26.08431\n",
      "MSE на шаге 111 26.01859\n",
      "MSE на шаге 121 25.97446\n",
      "MSE на шаге 131 25.94483\n",
      "MSE на шаге 141 25.92492\n",
      "MSE на шаге 151 25.91156\n",
      "MSE на шаге 161 25.90259\n",
      "MSE на шаге 171 25.89656\n",
      "MSE на шаге 181 25.89251\n",
      "MSE на шаге 191 25.88980\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x)\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "#     layer.weight.grad.zero_()\n",
    "#     layer.bias.grad.zero_()\n",
    "    \n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7088],\n",
       "        [-2.7079],\n",
       "        [-2.7120],\n",
       "        [-2.7109],\n",
       "        [-2.7058],\n",
       "        [-2.7104],\n",
       "        [-2.7078],\n",
       "        [-2.7138],\n",
       "        [-2.7085],\n",
       "        [-2.6886],\n",
       "        [-2.7112],\n",
       "        [-2.7167],\n",
       "        [-2.7099],\n",
       "        [-2.7070],\n",
       "        [-2.7189],\n",
       "        [-2.7043],\n",
       "        [-2.7015],\n",
       "        [-2.6970],\n",
       "        [-2.7104],\n",
       "        [-2.7030],\n",
       "        [-2.7107],\n",
       "        [-2.7059],\n",
       "        [-2.7023],\n",
       "        [-2.6938],\n",
       "        [-2.6980],\n",
       "        [-2.7100],\n",
       "        [-2.7036],\n",
       "        [-2.7149],\n",
       "        [-2.6986],\n",
       "        [-2.7071],\n",
       "        [-2.7058],\n",
       "        [-2.7120],\n",
       "        [-2.7075],\n",
       "        [-2.7024],\n",
       "        [-2.6972],\n",
       "        [-2.7002],\n",
       "        [-2.7231],\n",
       "        [-2.6898],\n",
       "        [-2.7025],\n",
       "        [-2.7044],\n",
       "        [-2.6971],\n",
       "        [-2.7054],\n",
       "        [-2.7004],\n",
       "        [-2.7124],\n",
       "        [-2.7017],\n",
       "        [-2.7106],\n",
       "        [-2.7000],\n",
       "        [-2.7090],\n",
       "        [-2.7167],\n",
       "        [-2.7088],\n",
       "        [-2.7076],\n",
       "        [-2.7150],\n",
       "        [-2.7196],\n",
       "        [-2.6918],\n",
       "        [-2.7091],\n",
       "        [-2.7059],\n",
       "        [-2.7018],\n",
       "        [-2.7150],\n",
       "        [-2.7125],\n",
       "        [-2.7051],\n",
       "        [-2.7038],\n",
       "        [-2.7099],\n",
       "        [-2.7130],\n",
       "        [-2.6969],\n",
       "        [-2.7049],\n",
       "        [-2.7082],\n",
       "        [-2.6997],\n",
       "        [-2.6944],\n",
       "        [-2.7127],\n",
       "        [-2.6980],\n",
       "        [-2.7059],\n",
       "        [-2.7104],\n",
       "        [-2.7122],\n",
       "        [-2.6925],\n",
       "        [-2.6929],\n",
       "        [-2.7053],\n",
       "        [-2.7066],\n",
       "        [-2.7098],\n",
       "        [-2.7078],\n",
       "        [-2.7139],\n",
       "        [-2.7157],\n",
       "        [-2.6928],\n",
       "        [-2.6982],\n",
       "        [-2.7209],\n",
       "        [-2.7039],\n",
       "        [-2.6907],\n",
       "        [-2.7031],\n",
       "        [-2.6997],\n",
       "        [-2.7030],\n",
       "        [-2.7071],\n",
       "        [-2.7193],\n",
       "        [-2.7018],\n",
       "        [-2.7143],\n",
       "        [-2.7099],\n",
       "        [-2.7151],\n",
       "        [-2.7126],\n",
       "        [-2.7101],\n",
       "        [-2.7095],\n",
       "        [-2.7177],\n",
       "        [-2.7162],\n",
       "        [-2.7027],\n",
       "        [-2.7045],\n",
       "        [-2.7027],\n",
       "        [-2.7069],\n",
       "        [-2.7110],\n",
       "        [-2.6906],\n",
       "        [-2.7140],\n",
       "        [-2.7019],\n",
       "        [-2.7195],\n",
       "        [-2.7120],\n",
       "        [-2.7086],\n",
       "        [-2.7118],\n",
       "        [-2.7110],\n",
       "        [-2.6942],\n",
       "        [-2.6955],\n",
       "        [-2.7129],\n",
       "        [-2.7076],\n",
       "        [-2.7096],\n",
       "        [-2.6928],\n",
       "        [-2.6915],\n",
       "        [-2.7045],\n",
       "        [-2.7129],\n",
       "        [-2.6942],\n",
       "        [-2.7100],\n",
       "        [-2.7206],\n",
       "        [-2.7065],\n",
       "        [-2.7167],\n",
       "        [-2.7055],\n",
       "        [-2.6992],\n",
       "        [-2.7113],\n",
       "        [-2.6984],\n",
       "        [-2.7078],\n",
       "        [-2.7183],\n",
       "        [-2.7057],\n",
       "        [-2.6910],\n",
       "        [-2.7093],\n",
       "        [-2.7005],\n",
       "        [-2.7210],\n",
       "        [-2.7101],\n",
       "        [-2.7119],\n",
       "        [-2.7082],\n",
       "        [-2.7154],\n",
       "        [-2.7021],\n",
       "        [-2.7039],\n",
       "        [-2.6924],\n",
       "        [-2.6987],\n",
       "        [-2.7127],\n",
       "        [-2.7135],\n",
       "        [-2.6981],\n",
       "        [-2.7071],\n",
       "        [-2.7004],\n",
       "        [-2.7141],\n",
       "        [-2.6992],\n",
       "        [-2.7064],\n",
       "        [-2.7119],\n",
       "        [-2.7017],\n",
       "        [-2.7058],\n",
       "        [-2.7155],\n",
       "        [-2.7046],\n",
       "        [-2.7001],\n",
       "        [-2.6969],\n",
       "        [-2.7090],\n",
       "        [-2.6991],\n",
       "        [-2.7167],\n",
       "        [-2.7046],\n",
       "        [-2.7015],\n",
       "        [-2.7059],\n",
       "        [-2.7006],\n",
       "        [-2.7194],\n",
       "        [-2.7043],\n",
       "        [-2.7152],\n",
       "        [-2.7157],\n",
       "        [-2.7108],\n",
       "        [-2.7070],\n",
       "        [-2.7134],\n",
       "        [-2.7156],\n",
       "        [-2.7064],\n",
       "        [-2.7209],\n",
       "        [-2.7066],\n",
       "        [-2.6901],\n",
       "        [-2.7125],\n",
       "        [-2.7185],\n",
       "        [-2.7109],\n",
       "        [-2.7128],\n",
       "        [-2.7025],\n",
       "        [-2.7033],\n",
       "        [-2.6938],\n",
       "        [-2.7074],\n",
       "        [-2.7095],\n",
       "        [-2.7045],\n",
       "        [-2.7134],\n",
       "        [-2.7028],\n",
       "        [-2.7082],\n",
       "        [-2.7197],\n",
       "        [-2.6969],\n",
       "        [-2.7182],\n",
       "        [-2.6991],\n",
       "        [-2.7181],\n",
       "        [-2.6956],\n",
       "        [-2.7005],\n",
       "        [-2.7142],\n",
       "        [-2.7112],\n",
       "        [-2.6964],\n",
       "        [-2.7115],\n",
       "        [-2.7076],\n",
       "        [-2.7116],\n",
       "        [-2.7007],\n",
       "        [-2.7088],\n",
       "        [-2.6934],\n",
       "        [-2.7024],\n",
       "        [-2.7134],\n",
       "        [-2.7168],\n",
       "        [-2.6966],\n",
       "        [-2.7109],\n",
       "        [-2.6977],\n",
       "        [-2.6935],\n",
       "        [-2.7097],\n",
       "        [-2.7132],\n",
       "        [-2.7229],\n",
       "        [-2.7126],\n",
       "        [-2.6987],\n",
       "        [-2.7003],\n",
       "        [-2.6982],\n",
       "        [-2.7196],\n",
       "        [-2.7088],\n",
       "        [-2.7130],\n",
       "        [-2.7143],\n",
       "        [-2.6976],\n",
       "        [-2.7062],\n",
       "        [-2.6993],\n",
       "        [-2.7125],\n",
       "        [-2.7081],\n",
       "        [-2.6984],\n",
       "        [-2.6946],\n",
       "        [-2.7191],\n",
       "        [-2.7159],\n",
       "        [-2.7103],\n",
       "        [-2.7192],\n",
       "        [-2.7127],\n",
       "        [-2.7028],\n",
       "        [-2.7032],\n",
       "        [-2.7093],\n",
       "        [-2.6950],\n",
       "        [-2.6954],\n",
       "        [-2.7115],\n",
       "        [-2.7025],\n",
       "        [-2.7217],\n",
       "        [-2.7045],\n",
       "        [-2.6922],\n",
       "        [-2.7053],\n",
       "        [-2.7117],\n",
       "        [-2.7137],\n",
       "        [-2.7063],\n",
       "        [-2.7056],\n",
       "        [-2.7121],\n",
       "        [-2.6985],\n",
       "        [-2.7006],\n",
       "        [-2.6965],\n",
       "        [-2.7089],\n",
       "        [-2.6933],\n",
       "        [-2.7023],\n",
       "        [-2.7016],\n",
       "        [-2.7085],\n",
       "        [-2.7145],\n",
       "        [-2.7040],\n",
       "        [-2.7114],\n",
       "        [-2.7135],\n",
       "        [-2.7049],\n",
       "        [-2.7104],\n",
       "        [-2.7140],\n",
       "        [-2.7174],\n",
       "        [-2.7097],\n",
       "        [-2.6899],\n",
       "        [-2.7030],\n",
       "        [-2.7066],\n",
       "        [-2.6989],\n",
       "        [-2.7127],\n",
       "        [-2.7135],\n",
       "        [-2.7203],\n",
       "        [-2.7071],\n",
       "        [-2.7006],\n",
       "        [-2.7004],\n",
       "        [-2.7045],\n",
       "        [-2.7128],\n",
       "        [-2.7029],\n",
       "        [-2.7083],\n",
       "        [-2.7030],\n",
       "        [-2.7181],\n",
       "        [-2.7030],\n",
       "        [-2.7064],\n",
       "        [-2.7025],\n",
       "        [-2.7011],\n",
       "        [-2.7231],\n",
       "        [-2.7040],\n",
       "        [-2.7226],\n",
       "        [-2.7017],\n",
       "        [-2.7128],\n",
       "        [-2.7208],\n",
       "        [-2.7074],\n",
       "        [-2.6964]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 300])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x) - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7088, -2.7079, -2.7120, -2.7109, -2.7058, -2.7104, -2.7078, -2.7138,\n",
       "        -2.7085, -2.6886, -2.7112, -2.7167, -2.7099, -2.7070, -2.7189, -2.7043,\n",
       "        -2.7015, -2.6970, -2.7104, -2.7030, -2.7107, -2.7059, -2.7023, -2.6938,\n",
       "        -2.6980, -2.7100, -2.7036, -2.7149, -2.6986, -2.7071, -2.7058, -2.7120,\n",
       "        -2.7075, -2.7024, -2.6972, -2.7002, -2.7231, -2.6898, -2.7025, -2.7044,\n",
       "        -2.6971, -2.7054, -2.7004, -2.7124, -2.7017, -2.7106, -2.7000, -2.7090,\n",
       "        -2.7167, -2.7088, -2.7076, -2.7150, -2.7196, -2.6918, -2.7091, -2.7059,\n",
       "        -2.7018, -2.7150, -2.7125, -2.7051, -2.7038, -2.7099, -2.7130, -2.6969,\n",
       "        -2.7049, -2.7082, -2.6997, -2.6944, -2.7127, -2.6980, -2.7059, -2.7104,\n",
       "        -2.7122, -2.6925, -2.6929, -2.7053, -2.7066, -2.7098, -2.7078, -2.7139,\n",
       "        -2.7157, -2.6928, -2.6982, -2.7209, -2.7039, -2.6907, -2.7031, -2.6997,\n",
       "        -2.7030, -2.7071, -2.7193, -2.7018, -2.7143, -2.7099, -2.7151, -2.7126,\n",
       "        -2.7101, -2.7095, -2.7177, -2.7162, -2.7027, -2.7045, -2.7027, -2.7069,\n",
       "        -2.7110, -2.6906, -2.7140, -2.7019, -2.7195, -2.7120, -2.7086, -2.7118,\n",
       "        -2.7110, -2.6942, -2.6955, -2.7129, -2.7076, -2.7096, -2.6928, -2.6915,\n",
       "        -2.7045, -2.7129, -2.6942, -2.7100, -2.7206, -2.7065, -2.7167, -2.7055,\n",
       "        -2.6992, -2.7113, -2.6984, -2.7078, -2.7183, -2.7057, -2.6910, -2.7093,\n",
       "        -2.7005, -2.7210, -2.7101, -2.7119, -2.7082, -2.7154, -2.7021, -2.7039,\n",
       "        -2.6924, -2.6987, -2.7127, -2.7135, -2.6981, -2.7071, -2.7004, -2.7141,\n",
       "        -2.6992, -2.7064, -2.7119, -2.7017, -2.7058, -2.7155, -2.7046, -2.7001,\n",
       "        -2.6969, -2.7090, -2.6991, -2.7167, -2.7046, -2.7015, -2.7059, -2.7006,\n",
       "        -2.7194, -2.7043, -2.7152, -2.7157, -2.7108, -2.7070, -2.7134, -2.7156,\n",
       "        -2.7064, -2.7209, -2.7066, -2.6901, -2.7125, -2.7185, -2.7109, -2.7128,\n",
       "        -2.7025, -2.7033, -2.6938, -2.7074, -2.7095, -2.7045, -2.7134, -2.7028,\n",
       "        -2.7082, -2.7197, -2.6969, -2.7182, -2.6991, -2.7181, -2.6956, -2.7005,\n",
       "        -2.7142, -2.7112, -2.6964, -2.7115, -2.7076, -2.7116, -2.7007, -2.7088,\n",
       "        -2.6934, -2.7024, -2.7134, -2.7168, -2.6966, -2.7109, -2.6977, -2.6935,\n",
       "        -2.7097, -2.7132, -2.7229, -2.7126, -2.6987, -2.7003, -2.6982, -2.7196,\n",
       "        -2.7088, -2.7130, -2.7143, -2.6976, -2.7062, -2.6993, -2.7125, -2.7081,\n",
       "        -2.6984, -2.6946, -2.7191, -2.7159, -2.7103, -2.7192, -2.7127, -2.7028,\n",
       "        -2.7032, -2.7093, -2.6950, -2.6954, -2.7115, -2.7025, -2.7217, -2.7045,\n",
       "        -2.6922, -2.7053, -2.7117, -2.7137, -2.7063, -2.7056, -2.7121, -2.6985,\n",
       "        -2.7006, -2.6965, -2.7089, -2.6933, -2.7023, -2.7016, -2.7085, -2.7145,\n",
       "        -2.7040, -2.7114, -2.7135, -2.7049, -2.7104, -2.7140, -2.7174, -2.7097,\n",
       "        -2.6899, -2.7030, -2.7066, -2.6989, -2.7127, -2.7135, -2.7203, -2.7071,\n",
       "        -2.7006, -2.7004, -2.7045, -2.7128, -2.7029, -2.7083, -2.7030, -2.7181,\n",
       "        -2.7030, -2.7064, -2.7025, -2.7011, -2.7231, -2.7040, -2.7226, -2.7017,\n",
       "        -2.7128, -2.7208, -2.7074, -2.6964], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x).ravel() - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 60.51762\n",
      "MSE на шаге 2 38.10868\n",
      "MSE на шаге 3 25.79010\n",
      "MSE на шаге 4 18.25825\n",
      "MSE на шаге 5 13.35753\n",
      "MSE на шаге 6 10.06214\n",
      "MSE на шаге 7 7.80691\n",
      "MSE на шаге 8 6.24670\n",
      "MSE на шаге 9 5.15782\n",
      "MSE на шаге 10 4.39096\n",
      "MSE на шаге 11 3.84500\n",
      "MSE на шаге 12 3.45099\n",
      "MSE на шаге 13 3.16180\n",
      "MSE на шаге 14 2.94515\n",
      "MSE на шаге 15 2.77890\n",
      "MSE на шаге 16 2.64787\n",
      "MSE на шаге 17 2.54164\n",
      "MSE на шаге 18 2.45303\n",
      "MSE на шаге 19 2.37711\n",
      "MSE на шаге 20 2.31048\n",
      "MSE на шаге 21 2.25079\n",
      "MSE на шаге 31 1.83216\n",
      "MSE на шаге 41 1.56585\n",
      "MSE на шаге 51 1.38735\n",
      "MSE на шаге 61 1.26749\n",
      "MSE на шаге 71 1.18700\n",
      "MSE на шаге 81 1.13295\n",
      "MSE на шаге 91 1.09666\n",
      "MSE на шаге 101 1.07228\n",
      "MSE на шаге 111 1.05591\n",
      "MSE на шаге 121 1.04492\n",
      "MSE на шаге 131 1.03754\n",
      "MSE на шаге 141 1.03258\n",
      "MSE на шаге 151 1.02925\n",
      "MSE на шаге 161 1.02702\n",
      "MSE на шаге 171 1.02552\n",
      "MSE на шаге 181 1.02451\n",
      "MSE на шаге 191 1.02383\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([300, 5]), torch.Size([300]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 500\n",
    "step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 2052.12988\n",
      "MSE на шаге 2 606.21967\n",
      "MSE на шаге 3 207.55234\n",
      "MSE на шаге 4 82.47279\n",
      "MSE на шаге 5 40.03379\n",
      "MSE на шаге 6 24.82249\n",
      "MSE на шаге 7 19.02928\n",
      "MSE на шаге 8 16.58483\n",
      "MSE на шаге 9 15.35648\n",
      "MSE на шаге 10 14.58259\n",
      "MSE на шаге 11 13.98817\n",
      "MSE на шаге 12 13.47272\n",
      "MSE на шаге 13 12.99872\n",
      "MSE на шаге 14 12.55160\n",
      "MSE на шаге 15 12.12528\n",
      "MSE на шаге 16 11.71682\n",
      "MSE на шаге 17 11.32460\n",
      "MSE на шаге 18 10.94749\n",
      "MSE на шаге 19 10.58467\n",
      "MSE на шаге 20 10.23542\n",
      "MSE на шаге 51 3.92278\n",
      "MSE на шаге 101 1.43192\n",
      "MSE на шаге 151 1.02777\n",
      "MSE на шаге 201 0.95445\n",
      "MSE на шаге 251 0.93498\n",
      "MSE на шаге 301 0.92541\n",
      "MSE на шаге 351 0.91856\n",
      "MSE на шаге 401 0.91310\n",
      "MSE на шаге 451 0.90864\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "step_size = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 1960.22107\n",
      "MSE на шаге 2 1875.74646\n",
      "MSE на шаге 3 1824.76184\n",
      "MSE на шаге 4 1783.01770\n",
      "MSE на шаге 5 1737.57739\n",
      "MSE на шаге 6 1680.35474\n",
      "MSE на шаге 7 1608.41382\n",
      "MSE на шаге 8 1521.16003\n",
      "MSE на шаге 9 1422.86169\n",
      "MSE на шаге 10 1326.70837\n",
      "MSE на шаге 11 1245.95276\n",
      "MSE на шаге 12 1187.99561\n",
      "MSE на шаге 13 1150.47595\n",
      "MSE на шаге 14 1126.26050\n",
      "MSE на шаге 15 1110.28662\n",
      "MSE на шаге 16 1099.15491\n",
      "MSE на шаге 17 1090.28088\n",
      "MSE на шаге 18 1082.69885\n",
      "MSE на шаге 19 1076.90894\n",
      "MSE на шаге 20 1072.41516\n",
      "MSE на шаге 51 1040.33777\n",
      "MSE на шаге 101 1012.64685\n",
      "MSE на шаге 151 22.40335\n",
      "MSE на шаге 201 7.19394\n",
      "MSE на шаге 251 4.40598\n",
      "MSE на шаге 301 2.97970\n",
      "MSE на шаге 351 2.25579\n",
      "MSE на шаге 401 1.88717\n",
      "MSE на шаге 451 1.69860\n",
      "MSE на шаге 501 1.59878\n",
      "MSE на шаге 551 1.54166\n",
      "MSE на шаге 601 1.50237\n",
      "MSE на шаге 651 1.47228\n",
      "MSE на шаге 701 1.44641\n",
      "MSE на шаге 751 1.42264\n",
      "MSE на шаге 801 1.40072\n",
      "MSE на шаге 851 1.38052\n",
      "MSE на шаге 901 1.36144\n",
      "MSE на шаге 951 1.34315\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=n_features, out_features=3)\n",
    "layer2 = nn.Linear(in_features=3, out_features=1)\n",
    "activation = nn.ReLU()\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer2(activation(layer1(x))).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer1.weight -= layer1.weight.grad * step_size\n",
    "        layer1.bias -= layer1.bias.grad * step_size\n",
    "        layer2.weight -= layer2.weight.grad * step_size\n",
    "        layer2.bias -= layer2.bias.grad * step_size\n",
    "\n",
    "    layer1.zero_grad()\n",
    "    layer2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seminar 1. Intro to DL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
